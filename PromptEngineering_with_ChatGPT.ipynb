{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "id": "XtgF7EulLWeZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a8eedec-54cd-40e1-b221-8a82ffaf3f2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.35.7-py3-none-any.whl (327 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m327.5/327.5 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.7.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.6.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.18.4)\n",
            "Installing collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 openai-1.35.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompt Engineering With ChatGpt"
      ],
      "metadata": {
        "id": "BURsHPncJ3ZN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai"
      ],
      "metadata": {
        "id": "AxCFV07kLWbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f=open(\"/content/openI_key1.txt\")\n",
        "key=f.read()\n",
        "os.environ[\"OPENAI_API_KEY\"]=key"
      ],
      "metadata": {
        "id": "OoMf5XlgLWYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=\"gpt-3.5-turbo\"\n"
      ],
      "metadata": {
        "id": "OmpkneKiLWVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client=openai.OpenAI()\n"
      ],
      "metadata": {
        "id": "4z5ZG-VjLWSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages=[{\"role\":\"user\",\"content\":\"Tell me the scope of data science in five bullet points\"}]\n",
        "response=client.chat.completions.create(model=model,messages=messages)"
      ],
      "metadata": {
        "id": "madUaHtvLWPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "id": "HghvpthtLWMw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86e2700f-e525-4b40-c9b4-8c2ec8564ffe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatCompletion(id='chatcmpl-9fRQxRzbgyhcil8TFb408UEQM8Czq', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='1. Increasing demand for data-driven insights in various industries such as healthcare, finance, marketing, and technology.\\n2. Growing opportunities for professionals with expertise in machine learning, statistical analysis, data visualization, and programming.\\n3. Introduction of new technologies and tools such as artificial intelligence, data mining, and big data processing.\\n4. Demand for data scientists to develop and implement data-driven strategies for business growth, risk management, and decision-making.\\n5. Potential for data science to drive innovation, optimize processes, and improve overall business performance.', role='assistant', function_call=None, tool_calls=None))], created=1719663931, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=107, prompt_tokens=18, total_tokens=125))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "SvKGZQnmLWJR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ba67af5-5e6a-46fa-d9b2-f6d885ba75ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Increasing demand for data-driven insights in various industries such as healthcare, finance, marketing, and technology.\n",
            "2. Growing opportunities for professionals with expertise in machine learning, statistical analysis, data visualization, and programming.\n",
            "3. Introduction of new technologies and tools such as artificial intelligence, data mining, and big data processing.\n",
            "4. Demand for data scientists to develop and implement data-driven strategies for business growth, risk management, and decision-making.\n",
            "5. Potential for data science to drive innovation, optimize processes, and improve overall business performance.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### defining a function that accepts prompt as input and return response"
      ],
      "metadata": {
        "id": "S_rqid9hwMlp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_answer(prompt,model=\"gpt-3.5-turbo\"):\n",
        "  messages=[{\"role\":\"user\",\"content\":prompt}]\n",
        "  response=openai.chat.completions.create(model=model,messages=messages)\n",
        "  return response.choices[0].message.content"
      ],
      "metadata": {
        "id": "E4CCXlQcv8ZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=\"What are skills needed as a Data scientist\"\n",
        "print(get_answer(prompt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rb_qi5wS0axe",
        "outputId": "868e775a-0dfb-4de4-f610-77824233c05b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Data Analysis: Data scientists need to be proficient in analyzing large amounts of data to extract meaningful insights and patterns.\n",
            "\n",
            "2. Programming: Proficiency in programming languages such as Python, R, SQL, and Java is essential for data scientists to manipulate and analyze data.\n",
            "\n",
            "3. Machine Learning: Data scientists need to have a strong understanding of machine learning algorithms and techniques to build predictive models and analyze data.\n",
            "\n",
            "4. Data Visualization: Data scientists should be able to effectively communicate their findings through data visualization tools like Tableau, PowerBI, and Matplotlib.\n",
            "\n",
            "5. Statistics: A strong foundation in statistics is crucial for data scientists to properly interpret data, perform hypothesis testing, and make informed decisions.\n",
            "\n",
            "6. Domain Knowledge: Data scientists should have knowledge of the industry they are working in, as well as a deep understanding of the data they are analyzing.\n",
            "\n",
            "7. Data Cleaning and Preprocessing: Data scientists need to be skilled in cleaning and preprocessing data to ensure its quality and reliability for analysis.\n",
            "\n",
            "8. Problem-Solving: Data scientists should be able to think critically and creatively to solve complex problems and provide data-driven solutions.\n",
            "\n",
            "9. Communication: Data scientists should be able to effectively communicate their findings and insights to stakeholders and non-technical audiences.\n",
            "\n",
            "10. Continuous Learning: Data science is a rapidly evolving field, and data scientists need to continuously update their skills and knowledge to stay relevant.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building Conversation  with the chatGPT API\n",
        "\n",
        "- Here we are going to involve the previous prompts input and responses to enable the chatgpt to get most accurate responses"
      ],
      "metadata": {
        "id": "EGPHaVmdMHz0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_1=\"Explain naive Bayes\"\n",
        "\n",
        "\n",
        "prompt_1_output=get_answer(prompt_1)\n",
        "print(prompt_1_output)"
      ],
      "metadata": {
        "id": "yxav6b_x0ssm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43ef5217-659a-48e0-ac3a-c22351300188"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive Bayes is a supervised machine learning algorithm that uses the Bayes' theorem with a \"naive\" assumption that all features are independent of each other. Despite this simplified assumption, Naive Bayes is known for its simplicity and efficiency in classification tasks.\n",
            "\n",
            "The algorithm works by calculating the probability of a data point belonging to a certain class based on the probabilities of each feature occurring in that class. It then selects the class with the highest probability as the predicted class for the data point.\n",
            "\n",
            "Naive Bayes is commonly used in text classification tasks, such as spam detection and sentiment analysis, due to its speed and ability to handle large datasets with high dimensions. It is also robust to irrelevant features and can work well even with limited training data.\n",
            "\n",
            "Overall, Naive Bayes is a powerful and widely-used algorithm in the field of machine learning, particularly in tasks that involve classifying data into categories.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_2=\"Give 10 applications of the above \""
      ],
      "metadata": {
        "id": "UEdNHFP4_M5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### including all the first prompt output also along with the first and second prompt\n",
        "messages=[{\"role\":\"user\",\"content\":prompt_1},\n",
        "          {\"role\":\"assistant\",\"content\":prompt_1_output},\n",
        "          {\"role\":\"user\",\"content\":prompt_2}]"
      ],
      "metadata": {
        "id": "n3OznMo8_M2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response=openai.chat.completions.create(model=\"gpt-3.5-turbo\",messages=messages)\n",
        "prompt_2_output=response.choices[0].message.content\n",
        "print(prompt_2_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPRwCAiW_MyN",
        "outputId": "3c154aa5-e547-4c6b-80de-33b9156f3d96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Spam detection in email filtering\n",
            "2. Sentiment analysis in social media monitoring\n",
            "3. Document categorization in information retrieval\n",
            "4. Fraud detection in financial transactions\n",
            "5. Medical diagnosis based on symptoms\n",
            "6. Customer segmentation in marketing\n",
            "7. News categorization in news classification\n",
            "8. Recommendation systems in e-commerce\n",
            "9. Predictive text input in mobile devices\n",
            "10. Customer churn prediction in telecom and subscription-based services\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_3=\"Explain one of the above applications in detail \"\n",
        "\n",
        "\n",
        "##including previous history along with third prompt\n",
        "messages=[{\"role\":\"user\",\"content\":prompt_1},\n",
        "          {\"role\":\"assistant\",\"content\":prompt_1_output},\n",
        "          {\"role\":\"user\",\"content\":prompt_2},\n",
        "          {\"role\":\"assistant\",\"content\":prompt_2_output},\n",
        "          {\"role\":\"user\",\"content\":prompt_3}]\n",
        "\n",
        "\n",
        "## getting the response\n",
        "response=openai.chat.completions.create(model=\"gpt-3.5-turbo\",messages=messages)\n",
        "prompt_3_output=response.choices[0].message.content\n",
        "\n",
        "print(prompt_3_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9tGtzQO_MuV",
        "outputId": "525717b8-6290-44f2-8dd0-2f3107220a7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One of the applications mentioned above is spam detection in email filtering using Naive Bayes algorithm. \n",
            "\n",
            "Spam detection is a common problem faced by email users, as unwanted emails (spam) can clutter their inbox and potentially contain harmful links or information. The goal of spam detection is to automatically classify incoming emails as either spam or non-spam (ham).\n",
            "\n",
            "The Naive Bayes algorithm is well-suited for this task due to its efficiency and ability to handle large amounts of text data. In this application, each email is represented as a set of features, typically words or tokens present in the email content. The algorithm calculates the probability of an email being spam or ham based on the occurrence of these features in the training data.\n",
            "\n",
            "During the training phase, the algorithm learns the likelihood of each feature (word or token) occurring in spam emails and non-spam emails. It then calculates the conditional probability of an email being spam or ham given the occurrence of each feature. These probabilities are used to predict the class of incoming emails during the testing phase.\n",
            "\n",
            "When a new email is received, the Naive Bayes algorithm calculates the probability of the email being spam or ham based on the conditional probabilities learned during training. The email is then classified as either spam or ham based on the class with the highest probability.\n",
            "\n",
            "Overall, spam detection using Naive Bayes algorithm is an effective and widely-used method for automatically filtering out unwanted emails and improving the user experience for email users.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Creating a chat function to make conversation with chat gpt"
      ],
      "metadata": {
        "id": "u35gzs7DHL12"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history=[]\n",
        "\n",
        "def chat_model(user_prompt,is_clear=False):\n",
        "\n",
        "  global history\n",
        "  if is_clear:\n",
        "    history=[]\n",
        "\n",
        "  input_message={\"role\":\"user\",\"content\":user_prompt}\n",
        "  history.append(input_message)\n",
        "\n",
        "  response=openai.chat.completions.create(model=\"gpt-3.5-turbo\",messages=history)\n",
        "  response=response.choices[0].message.content\n",
        "  response_message={\"role\":\"assistant\",\"content\":response}\n",
        "  history.append(response_message)\n",
        "\n",
        "  return response\n",
        "\n"
      ],
      "metadata": {
        "id": "97IzyvXzGxZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_model(\"Explain the principle behind convolutional neural network\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "tVSWbvS5GxWP",
        "outputId": "9c56ab99-71b4-4153-b9d7-8c13ec3a8762"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Convolutional Neural Networks (CNNs) are a type of neural network that is most commonly used for analyzing visual imagery. The key principle behind CNNs is the application of convolution operations to the input images. \\n\\nIn a CNN, the input image is processed using a series of convolutional layers. Each layer consists of a set of learnable filters, which are applied to small regions of the input image. These filters detect features such as edges, corners, and textures in the image. \\n\\nDuring the convolution operation, the filter is applied to each overlapping region of the input image and a dot product is computed. This process creates a feature map, which highlights the presence of certain features in the input image. The size and the stride of the filter control the size of the output feature map and the amount of overlap between neighboring regions.\\n\\nThe output feature maps from the convolutional layers are then passed through non-linear activation functions, such as ReLU, to introduce non-linearity into the network. Pooling layers are also commonly used to downsample the feature maps and reduce the spatial dimensions of the input.\\n\\nFinally, the output of the convolutional layers is flattened and passed through one or more fully connected layers to perform classification or regression. The entire network is trained using an optimization algorithm, such as backpropagation, to learn the weights of the filters that best capture the underlying patterns in the input data.\\n\\nOverall, the principle behind CNNs is to leverage the spatial hierarchy of features in the input image to extract meaningful patterns and make accurate predictions.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_model(\"Who first introduced above\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "Xe6qYYK9GxSq",
        "outputId": "a8a714e6-198a-4deb-c929-74634c269bbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The concept of Convolutional Neural Networks (CNNs) was first introduced by Yann LeCun and his colleagues in the late 1980s. Yann LeCun, a prominent researcher in the field of deep learning and artificial intelligence, is considered one of the pioneers of CNNs. He published a seminal paper titled \"Gradient-Based Learning Applied to Document Recognition\" in 1998, which introduced the LeNet architecture, a type of CNN that was successfully used for handwritten digit recognition and other image classification tasks.\\n\\nSince then, CNNs have become a foundational technique in the field of computer vision, and have been widely adopted for a variety of tasks, such as image classification, object detection, and image segmentation. The development and advancement of CNNs have led to significant breakthroughs in the field of artificial intelligence and have revolutionized the way visual data is analyzed and interpreted.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OwdkG4h4GxPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A-ypVtpdGxLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-B70Dg6cGxFB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}